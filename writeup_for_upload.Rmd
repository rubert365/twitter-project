---
title: "Twitter Data Analysis"
output: 
  word_document:
    reference_docx: formatting.docx
    fig_caption: yes
---
Andy Hammes

Final Project for R/Python class

Project due May 16, 2016

### Study Background
Social media has changed how information is generated and disseminated around the world. Twitter is a great example of this since it allows anyone to potentially reach anyone else, and to distribute any (and all) thoughts they have to people at large.  This has changed almost every aspect of life in developed areas with internet from travel to food to politics, and public health is no exception to this.  Since disease is a major factor in people's lives it stands to reason that it should make its presence known in social media, and this could be of potential use to statisticians or other public health professionals who want to examine disease spread.  This project attempts to look several ways of how publically available Twitter Tweets can be used to look at the Zika virus.

Data for this project was pulled from Twitter using a Twitter API.  All tweets used here are (or were at the time they were pulled) publically available.  From examining several data pulls Twitter appears to publically cache Tweets for a limited period of time (approximately a week to 10 days) before they are not longer available so longer term analysis of zika spread was not possible.  

The Twitter API used to obtain the tweet data was the ```twitteR``` package in R.  This required authentication and using a developer account to make the interface on the Twitter end, and only required inputting the consumer key, consumer secret, access token, and access secret on the R side. All subsequent data manipulation, storage, cleaning, and analysis as well as figure and table generation were also done in R.  Finally this report was written in R markdown, ported via pandocs to Word.  

This report will focus on a few potential uses of the tweet data including spatial examination of the tweeters, temporal examination of wheen the tweets happened, and word parsing and sentiment analysis of the tweet itself.

```{r, eval = TRUE, include = FALSE}
#===========================================================================#
# loading libraries
#===========================================================================#
library(twitteR)
library(plyr)
library(rworldmap)
library(rworldxtra)
library(tm)
library(wordcloud)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggmap)
library(data.table)
library(pander)

options(stringsAsFactors = FALSE)

#===========================================================================#
# mapping code
#===========================================================================#
#---------------------------------------------------------------------------#
# reading in dataset
#---------------------------------------------------------------------------#
zika_full <- read.csv('zika_for_locations.csv')
newmap <- getMap(resolution = 'medium')

#---------------------------------------------------------------------------#
# plotting on a map by time
#---------------------------------------------------------------------------#
# getting a color palatte
colors <- rainbow(n = 24, start = 0, end = 0.75)

# making an hour variable
zika_full$hour <- substr(zika_full$created, 12, 13)
zika_full$hour <- as.numeric(zika_full$hour)

#===========================================================================#
# 
# text analysis
#
#===========================================================================#
#===========================================================================#
# word analysis
#   note this will make a VERY LARGE dataset, only do with much RAM 
#===========================================================================#
zika_long <- unnest_tokens(zika_full, word, text)

#---------------------------------------------------------------------------#
# sentiment analysis
#---------------------------------------------------------------------------#
bing <- sentiments %>%
  filter(lexicon == 'bing') %>%
  select(-score)

zika_sentiment <- zika_long %>%
  inner_join(bing) %>%
  count(screenName, sentiment) %>%
  spread(sentiment, n, fill  = 0) %>%
  mutate(sentiment = positive - negative)

#---------------------------------------------------------------------------#
# looking at distributions with a lower (and upper?) bound
#---------------------------------------------------------------------------#
zika_sentiment$sentiment[zika_sentiment$sentiment <= -10] <- -10
zika_sentiment$sentiment[zika_sentiment$sentiment >= 10] <- 10

#---------------------------------------------------------------------------#
# plotting by sentiment
#---------------------------------------------------------------------------#
# getting the sentiment and full dataset merged by person
zika_sent_full <- merge(zika_full, zika_sentiment, all.x = TRUE,
                        by.x = 'screenName', by.y = 'screenName')
zika_sent_plot <- zika_sent_full[!is.na(zika_sent_full$longitude), ]
zika_sent_plot <- zika_sent_plot[!is.na(zika_sent_plot$sentiment), ]

#===========================================================================#
# making a table of the most used words within the bing lexicon
#===========================================================================#
zika_count <- zika_long %>% count(word, sort = TRUE)
zika_count <- zika_count[zika_count$word %in% c('zika', bing$word), ]

names(zika_count) <- c('word', 'count')

#===========================================================================#
# making a word-pairing table
#===========================================================================#
#---------------------------------------------------------------------------#
# making word pairing datasets of words paired with zika
#---------------------------------------------------------------------------#
zika_pairs <- zika_long %>% 
  pair_count(screenName, word, sort = TRUE)
zika_pairs <- zika_pairs[((zika_pairs$value1 %in% bing$word) & (zika_pairs$value2 %in% bing$word)) | 
                           (zika_pairs$value1 == 'zika' & (zika_pairs$value2 %in% bing$word)) |
                           ((zika_pairs$value1 %in% bing$word) & zika_pairs$value2 == 'zika'), ]

names(zika_pairs) <- c('word 1', 'word 2', 'count')

#===========================================================================#
# table of heavy tweeters
#===========================================================================#
zika_datatable <- data.table(zika_sent_full)
zika_coll <- zika_datatable[ ,
                             list(
                               tweets = length(text),
                               sentiment = mean(sentiment),
                               longitude = mean(longitude),
                               latitude = mean(latitude)
                             ),
                             by = screenName]

# sorting zika_coll
zika_coll <- zika_coll[order(zika_coll$tweets, decreasing = TRUE)]

#===========================================================================#
#
# time based analysis
#
#===========================================================================#
zika_time <- zika_sent_full

#===========================================================================#
# getting day/time subsets for use next
#===========================================================================#
#---------------------------------------------------------------------------#
# getting seprate days, minutes, seconds, hours, months...
#---------------------------------------------------------------------------#
zika_time$seconds <- as.numeric(substr(zika_time$created, 18, 19))
zika_time$minutes <- as.numeric(substr(zika_time$created, 15, 16))
zika_time$days <- as.numeric(substr(zika_time$created, 9, 10))
zika_time$months <- as.numeric(substr(zika_time$created, 6, 7))
zika_time$date <- substr(zika_time$created, 1, 10)
zika_time$date <- as.Date(zika_time$date, format = '%Y-%m-%d')

#---------------------------------------------------------------------------#
# getting weekdays
#---------------------------------------------------------------------------#
zika_time$weekday <- weekdays(zika_time$date, abbreviate = TRUE)
zika_time$weekdayn <- 0
zika_time$weekdayn[zika_time$weekday == 'Mon'] <- 1
zika_time$weekdayn[zika_time$weekday == 'Tue'] <- 2
zika_time$weekdayn[zika_time$weekday == 'Wed'] <- 3
zika_time$weekdayn[zika_time$weekday == 'Thu'] <- 4
zika_time$weekdayn[zika_time$weekday == 'Fri'] <- 5
zika_time$weekdayn[zika_time$weekday == 'Sat'] <- 6
zika_time$weekdayn[zika_time$weekday == 'Sun'] <- 7

#---------------------------------------------------------------------------#
# combining time measurements to get larger time scales
#---------------------------------------------------------------------------#
zika_time$min_sec <- zika_time$minutes + (zika_time$seconds / 60)
zika_time$hour_min <- zika_time$hour + (zika_time$min_sec / 60)
zika_time$day_hm <- zika_time$days + (zika_time$hour_min / 24)
zika_time$month_dhm <- zika_time$months + (zika_time$day_hm / 30)

zika_time$weektime <- zika_time$weekdayn + (zika_time$hour_min/24)
```

## Methods
All data was imported using the ```twitteR``` package in R.  Datasets were imported in batches of about a week from when the project was assigned, totaling three large data pulls.  In the ```twitteR``` package the ```searchTwitter()``` function was used to import all tweets which included 'zika' in them, with an arbitrary high number of maximum tweets forcing it to import all tweets available within the API's cached tweets.  Experimentation showed that the ```searchTwitter()``` functionality was not case sensitive.  The imported tweet data included the raw text of the tweet itself, whether the tweet was favorited, the number of favorites it recieved, whether it was a reply to an SN, when the tweet was created (to the second), whether the tweet had to be truncated, whther it was a reply to an SID, a global id of the tweet, whether it was a reply to a UI, the type of interface Twitter had with the device used to tweet, the screen name of the tweeter, the retweet count, whether it was retweeted, whether it was a retweet itself, and the latitude and longitute of the tweeter when the tweet was made.

Optimally to examine the location of the tweet the latitude and longitude within Twitter would be used, unfortunately this feature is optional, and is present in well under 1% of the tweets.  Thus a workaround was used by searching by screen name for the account and getting the location provided by the user (if any).  This location was then imported into a google API which matched the location to a latitude/longitude on Earth.  This was then used as the location in subsequent analysis.

To examine the time of the tweet when the tweet was created was used.  Unlike the location information this was known for all the available tweets.  Time was examined in several ways to look for potential patterns.  These included daily (when in a 24 hour day the tweet occured), weekly (when in a 7 day week the tweet occured), and overall for the approximately 3 week period.

To examine the text of the tweet itself functions were used from the ```tm```, ```wordcloud```, and ```tidytext``` packages in R, with ```tidytext``` being the most used.  The first technique used was sentiment analysis which looked at the text of each individual's tweet and used the bing lexicon to assign the words positive, negative, or neutral connotations.  These values were totaled for each tweet to get the sentiment of that tweet, which was then totaled by person.  Individual word frequencies were also examined to look at the most used words overall, and the most paired words within the tweets.  For all of the text based analyses the only language which was able to be used was English which somewhat limited text from Latin and South America, and extremely limited the majority from Asia.

## Results
Plots of the tweets overall are shown in Figure 1, and plots of tweets by time of the tweet, and tweets by sentiment analysis score are shown in Figure 2.  Looking at the zika tweets overall, tweets appear to largely be in North America and Europe, with a particularily high density in Southern Brazil near Rio de Janero.  There are less tweets than might be expected in Asia from its population, but that is likely due to limitations of Twitter usage beyond the Americas and Europe. Looking at Zika tweets by time, there does not appear to be any relation between location and the local time of the tweeting. Zika tweets by sentiment do appear to have some interesting patterns.  There are more negative tweets in the US and Brazil than the rest of the continents, and Eastern Europe is more oddly positive than the other locations.

Plots showing the number of tweets by different time divisions are shown in Figure 3.  The first panel shows the number of tweets by the time of the day.  This shows that most tweets are between about 3:00 PM and 11:30 PM, and the number of tweets decreases from midnight to 10:30 AM where it picks up again.  This agrees with intuition that most of the users of twitter say up later at night, and are not using it as much in the morning.  Looking at the second panel of tweets by day of the week shows the same trend as was in the hourly analysis with dips in the morning and peaks at night.  There also appears to be more tweeting on Saturday than any other day of the week, with substantially less on Sunday, and oddly high amounts on Tuesday.  Finally, looking at the tweets within the overall time frame the Tuesday bumps are not present, but the Saturday spikes are very obvious.

Table 1 shows the most common words from all the tweets.  The most common one is obviously 'zika' itself as that was the text used to find the tweets in the first place.  The subsequent most used words are somewhat obviously connected and are 'virus' and 'death'.  There are also many words which convey immediate concern such as 'emergency', 'threat', 'outbreak', 'crisis', 'fears', and 'epidemic'.  There are some inclusions which are odd such as 'like', and 'positive', which have good connotations, and 'sin' which does have a negative connotation but which may be worth investigating further.

Table 2 shows the most commonly paired words in all the tweets.  The words here are all 'zika' or 'virus' paired to words which are mostly in the individual word counts.  The similarity compared to the most used words overall is interesting as it may show that there are several buzzwords which are associated with zika which are repeated many times in many of the tweets.

Table 3 shows the most prolific tweeters overall.  Considering that only tweets were included which had zika over a 3 week timeframe there are some remarkably high tweet counts.  Several of the screen names are specifically zika based such as 'Pikachu Zika', 'Zika News', 'Halt Zika',  'ZikaVirusTopNews', etc.  There are also some more general health sounding ones such as 'KinolaiHealth', 'pharma global', 'health google', and 'health news11'.  All of the most prolific tweeters but one also tweeted mostly in English as they were available for sentiment analysis, and almost all were negative.  It is worth noting however that there were a few positive tweeters overall especially 'Zika oo' who might be worth investigating further.

## Conclusions
Overall, results from using Twitter tweet data to look at the spread of the zika virus were interesting, if not very surprising.  Spatial analysis of the tweets appeared to show a relationship between how immediate of a threat zika is to the sentiment of the tweets with America and Brazil being the most negative.  Looking at when zika tweets were sent was not very informative as the time frame was not long enough to capture when zika entered public knowlege, and likely only shows normal Twitter usage patterns.  Examination of the tweets themselves showed that the most common words used with zika were negative and had to do with epidemics or concerns with the disease spread, with a few odd positive words included as well.

There were several limitations to this analysis.  First, the Twitter API does not publically cache tweets over a week from when the data is pulled which severly limited the scope of the timeframe which could be examined.  Twitter also did not include the location of the vast majority of its users forcing the use of a less accurate method; and even this was limited as the Twitter API was throttled to a certain number of queries over a time period so acquiring user data was very time consuming, and ultimately had to be cut short before its conclusion.  The text analysis was also limited by the words having to be in English as well as its characters needing to be in regular ASCII test format. Finally the largest limitation from this data overall is it only can represent individuals who have Twitter, and thus who have internet access, so generalization to areas where this is not the case is problematic.

In conclusion, using Twitter for Health Sciences data may be a rewarding endevor.  The limited data acquired for this analysis did have some promising results which make sense with what is happening in the real world.  However, using this on a large scale is heavily limited by the ability to download the data itself from Twitter and the almost complete lack of usable spatial information included with the public data.

### Zika tweet locations
```{r, echo = FALSE, fig.width = 8, fig.height = 5, warning = FALSE}
newmap <- getMap(resolution = 'medium')
plot(newmap)
points(zika_full$longitude, zika_full$latitude, col = 'blue', cex = 0.05)
```

Figure 1: Zika tweet locations.  Each blue dot represents a tweet instance which was able to be mapped to a latitude and longitude pairing.  

### Zika tweets by time of day and sentiment
```{r, echo = FALSE, fig.width = 8, fig.height = 9}
par(mfrow = c(2, 1))
# plotting by time
plot(newmap, main = 'Zika Tweets by Time')
colors <- rainbow(n = 24, start = 0, end = 0.75)
for(i in 1:24){
  holder <- zika_full[zika_full$hour == i, ]
  points(holder$longitude, holder$latitude, col = colors[i], cex = 0.45)
}

# plotting by sentiment
plot(newmap, main = 'Zika Tweets by sentiment')
colors <- rainbow(length(unique(zika_sent_plot$sentiment)), start = 0, end = 0.75)
for(i in min(zika_sent_plot$sentiment):max(zika_sent_plot$sentiment)){
  holder <- zika_sent_plot[zika_sent_plot$sentiment == i, ]
  points(holder$longitude, holder$latitude, col = colors[i], cex = 0.5)
}
par(mfrow = c(1,1))
```

Figure 2: Zika tweets by time of day and by sentiment.  For the time of day plot the hour of the day of the tweet corresponds to color, with red being 12:00 AM through blue being 11:00 PM.  For the sentiment plot the color corresponds to the sentiment of the user at the location, with red being a lower (negative) sentiment through blue being a high (positive) sentiment.

### Zika Tweets by Time
```{r, echo = FALSE, fig.width = 5, fig.height = 8}
par(mfrow = c(3, 1))
hist(zika_time$hour_min, breaks = 100, xlab = 'hour', ylab = 'tweets', main = 'Zika Tweets by Hour of the Day')
hist(zika_time$weektime, breaks = 100, xlab = 'day of the week', ylab = 'tweets', main = 'Zika Tweets by Day of the Week')
hist(zika_time$month_dhm, breaks = 200, xlab = 'month', ylab = 'tweets', main = 'Zika Tweets Within Overall Timeframe')
par(mfrow = c(1,1))
```

### Table of Most Used Words
```{r, echo = FALSE, results = 'markdown'}
#---------------------------------------------------------------------------#
# making the table itself
#---------------------------------------------------------------------------#
pander(zika_count[1:30, ], caption = 'Table 1: Most common words in all the tweets.')
```

### Table of Most Paired Words
```{r, echo = FALSE, results = 'markdown'}
#---------------------------------------------------------------------------#
# making the table itself
#---------------------------------------------------------------------------#
pander(zika_pairs[1:30, ], caption = 'Table 2: Most paired sets of words in all the tweets.')
```

### Table of Prolific Tweeters
```{r, echo = FALSE, results = 'markdown'}
#---------------------------------------------------------------------------#
# making the table itself
#---------------------------------------------------------------------------#
pander(zika_coll[1:30, ], caption = 'Table 3: Most prolific tweeters within the tweets acquired.')
```

### References
Zika on Wikipedia - 
https://en.wikipedia.org/wiki/Zika_virus

R bloggers sentiment analysis by Fisseha Berhane - 
http://www.r-bloggers.com/sentiment-analysis-on-donald-trump-using-r-and-tableau/

Happy collaboration with Rmd to docx by Richard Layton - 
http://rmarkdown.rstudio.com/articles_docx.html

Getting started with twitteR in R by 'stathack' - 
http://www.r-bloggers.com/getting-started-with-twitter-in-r/

Mapping Twitter Followers in R by 'lucaspuente' - 
http://www.r-bloggers.com/mapping-twitter-followers-in-r/

Introduction to tidytext by Julia Silge and David Robinson - 
https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html

Advanced R by Hadley Wickham

### Appendix with R code
```{r, eval = FALSE}
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
#
# R code final
#   R code for documenting the final project for the R/Python course
#   By: Andy Hammes
#   Due: May 16, 2016
#
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#
#===========================================================================#
# loading libraries
#===========================================================================#
library(twitteR)
library(plyr)
library(rworldmap)
library(rworldxtra)
library(tm)
library(wordcloud)
library(tidytext)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggmap)
library(data.table)
library(pander)

options(stringsAsFactors = FALSE)

#===========================================================================#
# setting up Twitter Authentication
#   This code is commented out as it does not include the keys and secrets 
#   it would need to run
#===========================================================================#
setup_twitter_oauth(consumer_key = '-----',
                    consumer_secret = '-----',
                    access_token = '-----',
                    access_secret = '-----')

#===========================================================================#
# reading in the data from Twitter
#   This code is also commented out as it will not run without authentication
#===========================================================================#
zika_archive <- searchTwitter('zika', n = 100000000)
zika_archive <- ldply(zika_archive, function(x) x$toDataFrame() )
write.csv(zika_archive, 'zika_archive.csv')

#===========================================================================#
# reading in data already loaded from Twitter
#===========================================================================#
zika_raw_1 <- read.csv('-----/zika_archive.csv')
zika_raw_2 <- read.csv('-----/zika_archive.csv')
zika_raw_3 <- read.csv('-----/zika_archive.csv')

#===========================================================================#
# making a single dataset (combining the pulls)
#   Combining is done by making a key by person, time, and text
#   to ensure no duplicates if time frames overlap
#===========================================================================#
#---------------------------------------------------------------------------#
# making the key
#---------------------------------------------------------------------------#
zika_raw_1$key <- paste(zika_raw_1$screenName, zika_raw_1$created, zika_raw_1$text)
zika_raw_2$key <- paste(zika_raw_2$screenName, zika_raw_2$created, zika_raw_2$text)
zika_raw_3$key <- paste(zika_raw_3$screenName, zika_raw_3$created, zika_raw_3$text)

#---------------------------------------------------------------------------#
# binding the datasets together
#---------------------------------------------------------------------------#
zika_full <- rbind(zika_raw_1, zika_raw_2, zika_raw_3)

#---------------------------------------------------------------------------#
# removing duplicates
#---------------------------------------------------------------------------#
zika_full <- zika_full[!duplicated(zika_full$key), ]

#---------------------------------------------------------------------------#
# getting rid of the key variable
#---------------------------------------------------------------------------#
zika_full <- zika_full[-length(zika_full)]
zika_full <- zika_full[!is.na(zika_full$X), ]

#===========================================================================#
# getting locations of all possible
#===========================================================================#
#---------------------------------------------------------------------------#
# finding the locations of everyone
#   commented out here as this will not be logged into Twitter
#   note outer loop is to repeat for arbitrarily long to get 
#   around twitter API limiting
#---------------------------------------------------------------------------#
zika_full$new_locations <- NA
people <- unique(zika_full$screenName)
for(q in 1:100){  # loop to fill in missing as API resets
  for(i in 1:length(people)){  # looping through tweets
    holder <- zika_full[zika_full$screenName == people[i], ]
    holder <- holder[1, ]
    if(is.na(holder$new_locations)){
      person <- holder$screenName
      holder <- person
      person <- tryCatch(getUser(person),  # if possible get the user information 
                         # from the screen name, otherwise NA
                         error = function(err){
                           bad <- NA
                           return(bad)
                         }
      )
      if(!is.null(person)){
        if(!is.na(person)){# if possible get location of the person, otherwise NA
          location <- tryCatch(twitteR::location(person), 
                               error = function(err){
                                 bad <- NA
                                 return(bad)
                               }
          )
        }
      }
      if(!is.na(location)){  # if we can get the location of the person, getting
        # the lat/lon from google
        location <- tryCatch(geocode(location, output = 'latlon', source = 'dsk'),
                             error = function(err){
                               bad <- NA
                               return(bad)
                             }
        )
      }
      if(!is.na(location)){
        latitude_new <- location$lat
        longitude_new <- location$lon
      } else {  # if we were unable to get the location from Twitter NAa
        latitude_new <- NA
        longitude_new <- NA
      }
      to_return <- paste(latitude_new, ', ', longitude_new, sep = '')  # returning
      # lat/lon
      zika_full$new_locations[zika_full$screenName == people[i]] <- to_return
    }
    zika_full$new_locations[zika_full$new_locations == 'NA, NA'] <- NA
    print(paste('person:', i, 'of:', length(people)))
    if(i %% 50000 == 0){
      write.csv(zika_full, 'zika_locations.csv')
    }
  }
  write.csv(zika_full, 'zika_locations.csv')
}


#---------------------------------------------------------------------------#
# reading in the saved zika locations file
#---------------------------------------------------------------------------#
zika_full <- read.csv('failsafes/zika_locations 4.csv')

#===========================================================================#
# 
# mapping
#
#===========================================================================#
#---------------------------------------------------------------------------#
# separating the latitude and longitude in the new_locations column
#---------------------------------------------------------------------------#
for(i in 1:length(zika_full$text)){
  if(is.na(zika_full$latitude[i])){
    holder <- strsplit(zika_full$new_locations[i], ', ')
    zika_full$latitude[i] <- holder[[1]][1]
    zika_full$longitude[i] <- holder[[1]][2]
  }
  print(i)
}

#---------------------------------------------------------------------------#
# writing this to avoid doing the loop again
#---------------------------------------------------------------------------#
write.csv(zika_full, 'zika_for_locations.csv')

#---------------------------------------------------------------------------#
# reading in dataset
#---------------------------------------------------------------------------#
zika_full <- read.csv('zika_for_locations.csv')

#---------------------------------------------------------------------------#
# plotting on a map outright
#---------------------------------------------------------------------------#
newmap <- getMap(resolution = 'medium')
plot(newmap)
points(zika_full$longitude, zika_full$latitude, col = 'blue', cex = 0.05)

#---------------------------------------------------------------------------#
# plotting on a map by time
#---------------------------------------------------------------------------#
# getting a color palatte
colors <- rainbow(n = 24, start = 0, end = 0.75)

# making an hour variable
zika_full$hour <- substr(zika_full$created, 12, 13)
zika_full$hour <- as.numeric(zika_full$hour)

# plotting by time
plot(newmap)
for(i in 1:24){
  holder <- zika_full[zika_full$hour == i, ]
  points(holder$longitude, holder$latitude, col = colors[i], cex = 0.45)
}

#===========================================================================#
# 
# text analysis
#
#===========================================================================#
#===========================================================================#
# word analysis
#   note this will make a VERY LARGE dataset, only do with much RAM 
#===========================================================================#
zika_long <- unnest_tokens(zika_full, word, text)

#---------------------------------------------------------------------------#
# sentiment analysis
#---------------------------------------------------------------------------#
bing <- sentiments %>%
  filter(lexicon == 'bing') %>%
  select(-score)

zika_sentiment <- zika_long %>%
  inner_join(bing) %>%
  count(screenName, sentiment) %>%
  spread(sentiment, n, fill  = 0) %>%
  mutate(sentiment = positive - negative)

#---------------------------------------------------------------------------#
# looking at the extreme outlier
#---------------------------------------------------------------------------#
min_name <- zika_sentiment$screenName[zika_sentiment$sentiment == min(zika_sentiment$sentiment)]
min_tweet <- zika_full[zika_full$screenName == min_name, ]
max_name <- zika_sentiment$screenName[zika_sentiment$sentiment == max(zika_sentiment$sentiment)]
max_tweet <- zika_full[zika_full$screenName == max_name, ]

#---------------------------------------------------------------------------#
# looking at distributions with a lower (and upper?) bound
#---------------------------------------------------------------------------#
zika_sentiment$sentiment[zika_sentiment$sentiment <= -10] <- -10
zika_sentiment$sentiment[zika_sentiment$sentiment >= 10] <- 10

hist(zika_sentiment$sentiment, breaks = 20)

#---------------------------------------------------------------------------#
# plotting by sentiment
#---------------------------------------------------------------------------#
# getting the sentiment and full dataset merged by person
zika_sent_full <- merge(zika_full, zika_sentiment, all.x = TRUE,
                        by.x = 'screenName', by.y = 'screenName')
zika_sent_plot <- zika_sent_full[!is.na(zika_sent_full$longitude), ]
zika_sent_plot <- zika_sent_plot[!is.na(zika_sent_plot$sentiment), ]

# plotting
plot(newmap)
colors <- rainbow(length(unique(zika_sent_plot$sentiment)), start = 0, end = 0.75)
for(i in min(zika_sent_plot$sentiment):max(zika_sent_plot$sentiment)){
  holder <- zika_sent_plot[zika_sent_plot$sentiment == i, ]
  points(holder$longitude, holder$latitude, col = colors[i], cex = 0.5)
}

#===========================================================================#
# making a table of the most used words within the bing lexicon
#===========================================================================#
zika_count <- zika_long %>% count(word, sort = TRUE)
zika_count <- zika_count[zika_count$word %in% c('zika', bing$word), ]

#---------------------------------------------------------------------------#
# making the table itself
#---------------------------------------------------------------------------#
pander(zika_count[1:50, ], caption = 'Table 1: Most common words in all the tweets.')

#===========================================================================#
# making a word-pairing table
#===========================================================================#
#---------------------------------------------------------------------------#
# making word pairing datasets of words paired with zika
#---------------------------------------------------------------------------#
zika_pairs <- zika_long %>% 
  pair_count(screenName, word, sort = TRUE)
zika_pairs <- zika_pairs[((zika_pairs$value1 %in% bing$word) & (zika_pairs$value2 %in% bing$word)) | 
                           (zika_pairs$value1 == 'zika' & (zika_pairs$value2 %in% bing$word)) |
                           ((zika_pairs$value1 %in% bing$word) & zika_pairs$value2 == 'zika'), ]

#---------------------------------------------------------------------------#
# making the table itself
#---------------------------------------------------------------------------#
pander(zika_pairs[1:50, ], caption = 'Table 2: Most paired sets of words in all the tweets.')

#===========================================================================#
# table of heavy tweeters
#===========================================================================#
zika_datatable <- data.table(zika_sent_full)
zika_coll <- zika_datatable[ ,
                             list(
                               tweets = length(text),
                               sentiment = mean(sentiment),
                               longitude = mean(longitude),
                               latitude = mean(latitude)
                             ),
                             by = screenName]

# sorting zika_coll
zika_coll <- zika_coll[order(zika_coll$tweets, decreasing = TRUE)]

#---------------------------------------------------------------------------#
# making the table itself
#---------------------------------------------------------------------------#
pander(zika_coll[1:50, ], caption = 'Table 3: Most prolific tweeters within the tweets acquired.')

#===========================================================================#
#
# time based analysis
#
#===========================================================================#
zika_time <- zika_sent_full

#===========================================================================#
# getting day/time subsets for use next
#===========================================================================#
#---------------------------------------------------------------------------#
# getting seprate days, minutes, seconds, hours, months...
#---------------------------------------------------------------------------#
zika_time$seconds <- as.numeric(substr(zika_time$created, 18, 19))
zika_time$minutes <- as.numeric(substr(zika_time$created, 15, 16))
zika_time$days <- as.numeric(substr(zika_time$created, 9, 10))
zika_time$months <- as.numeric(substr(zika_time$created, 6, 7))
zika_time$date <- as.numeric(substr(zika_time$created, 1, 10))
zika_time$date <- as.Date(zika_time$date, format = '%Y-%m-%d')

#---------------------------------------------------------------------------#
# getting weekdays
#---------------------------------------------------------------------------#
zika_time$weekday <- weekdays(zika_time$date, abbreviate = TRUE)
zika_time$weekdayn <- 0
zika_time$weekdayn[zika_time$weekday == 'Mon'] <- 1
zika_time$weekdayn[zika_time$weekday == 'Tue'] <- 2
zika_time$weekdayn[zika_time$weekday == 'Wed'] <- 3
zika_time$weekdayn[zika_time$weekday == 'Thu'] <- 4
zika_time$weekdayn[zika_time$weekday == 'Fri'] <- 5
zika_time$weekdayn[zika_time$weekday == 'Sat'] <- 6
zika_time$weekdayn[zika_time$weekday == 'Sun'] <- 7

#---------------------------------------------------------------------------#
# combining time measurements to get larger time scales
#---------------------------------------------------------------------------#
zika_time$min_sec <- zika_time$minutes + (zika_time$seconds / 60)
zika_time$hour_min <- zika_time$hour + (zika_time$min_sec / 60)
zika_time$day_hm <- zika_time$days + (zika_time$hour_min / 24)
zika_time$month_dhm <- zika_time$months + (zika_time$day_hm / 30)

zika_time$weektime <- zika_time$weekdayn + (zika_time$hour_min/24)

#===========================================================================#
# plotting things
#===========================================================================#
hist(zika_time$weektime, breaks = 100, ylab = 'day of the week', xlab = 'tweets',
     main = 'Zika Tweets by Day of the Week')
hist(zika_time$month_dhm, breaks = 100, ylab = 'month', xlab = 'tweets',
     main = 'Zika Tweets Within Overall Timeframe')
hist(zika_time$hour_min, breaks = 100, ylab = 'hour', xlab = 'tweets',
     main = 'Zika Tweets by Hour of the Day')
```

```{r}
sessionInfo()
```

